


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error,accuracy_score,roc_auc_score,precision_score,recall_score
from math import sqrt
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras import regularizers

import keras_tuner as kt
from statsmodels.tsa.stattools import adfuller

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.arima_model import ARMA
from scipy.stats import boxcox
from statsmodels.tsa.seasonal import seasonal_decompose
from tensorflow.keras.metrics import Precision,Recall,AUC

from pandas.plotting import autocorrelation_plot
from time import time





df = pd.read_csv("prepared_data.csv")
df = df.set_index(["timestamp"])
np.random.seed(42)
df.index = pd.to_datetime(df.index)
   








df = df.asfreq("4H")


print(df.index.freq)





df["outcome"] = df["close"] + df["net_change"]
df





X = pd.DataFrame(df["close"][3:]) ## HERE I CHANGED 4 TO 3 BECAUSE NOW WE ARE LOGGING THE TEST SET LATER ON WE WILL HAVE TO REMOVE THAT ROW. OR IS IT BETTER TO FORWARDS FILL THAT ONE AND THE ONE ON THE TRAIN DATA
X





X.isnull().sum()






split_distance = int(0.9 * len(X))-3
train, test = X.iloc[:split_distance], X.iloc[split_distance:]


train.isnull().sum()





f"Train shape:{train.shape}"


f"Test shape:{test.shape}"





test


train





df["close_transformed"] = np.log(df["close"]).diff()
df.dropna(inplace = True)


df["close_transformed"]





plot_acf(df["close_transformed"],lags=10)
plt.show()





plot_pacf(df["close_transformed"],lags=10)
plt.show()


## Here we apply a log transformation to stabilize the variance of the data. 

### check that the dates and lengths of everything that needs to match up still does 


train


train = np.log(train)
print(train.isnull().sum())

train


test.head()


start = test.index[0]
end = test.index[-1]


start


end


test = np.log(test)
test = test.dropna()
test.index.freq


model_predictions = []

#number of test observations
n_test = len(test)



start_time = time()
counter = 0
for i in range(n_test):
    print(f"running {i} out of {n_test}")
    model = ARIMA(train,order = (4,1,0))
    model_fit = model.fit()
    output = model_fit.forecast()
    yhat = list(output)
    model_predictions.append(yhat)
    actual_test_value = list(test["close"])
    counter += 1
    if counter == 10:
        break
end_time = time()

print(f"Model fitting time:{end_time-start_time}")


print(model_fit.summary())





pred_start_date = pd.to_datetime(test.index[0])
pred_end_date =pd.to_datetime(test.index[-1])

print(f"Start date:{pred_start_date}\nEnd date:{pred_end_date}")





predictions = model_fit.predict(start = pred_start_date, end = pred_end_date)
residuals = test - predictions
residuals





plt.figure(figsize=(15,9))
plt.grid(True)
date_range = X[split_distance:].index
plt.plot(date_range,predictions, color ="blue",marker = "o", linestyle = "dashed",label = "BTC predicted price")
plt.plot(date_range,test, color ="red",label = "BTC test actual price")
plt.title("test vs predicted)
plt.xlabel("Date")
plt.ylabel("price")
plt.legend()
plt.show()


'''

PICK UP NOTES

FIGURE OUT THE ISSUE WITH FREQUENCY

FIGURE OUT HOW TO RUN AN ARIMA MODEL!

need to undo the log and the differencing of the  predictions if done before. do i need to log before even if i difference inside model

how does the data need to be preprocessed for ARIMA

in p,d,q does is it better to have no differencing first and have d as one or do the differencing first and then have d as 0?

instead of dropping rows after logging data use a forward or a back fill to keep the row


not sure how to make this work for binary classification maybe it should be a regression instead. in that case is it measuring the outcome val or the close price.
'''
